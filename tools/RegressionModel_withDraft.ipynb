{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Regression Model ##\n",
    "\n",
    "This script runs a pipeline to load data, format data, train the neural network, evaluate it's accuracy, and save the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tools\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import json\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "'''\n",
    "import multiprocessing as mp\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import sklearn.preprocessing as PP\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('/home/ada/Documents/HullParameterization')\n",
    "\n",
    "from HullParameterization import Hull_Parameterization as HP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CPU Multithreading\n",
    "\n",
    "def run_MAP_multiprocessing(func, argument_list, chunksize = None, show_prog = True):\n",
    "    \"\"\"Run function in parallel\n",
    "    Parameters\n",
    "    ----------\n",
    "    func:          function\n",
    "                    Python function to run in parallel.\n",
    "    argument_list: list [N]\n",
    "                    List of arguments to be passed to the function in each parallel run.\n",
    "            \n",
    "    show_prog:     boolean\n",
    "                    If true a progress bas will be displayed to show progress. Default: True.\n",
    "    Returns\n",
    "    -------\n",
    "    output:        list [N,]\n",
    "                    outputs of the function for the given arguments.\n",
    "    \"\"\"\n",
    "    #Reserve 2 threads for other Tasks\n",
    "    #pool = mp.Pool(processes=mp.cpu_count()-2)\n",
    "    \n",
    "    if show_prog:            \n",
    "        result_list_tqdm = []\n",
    "        for result in tqdm(pool.map(func=func, iterable=argument_list,chunksize=chunksize), total=len(argument_list),position=0, leave=True):\n",
    "            result_list_tqdm.append(result)\n",
    "    else:\n",
    "        result_list_tqdm = []\n",
    "        for result in pool.map(func=func, iterable=argument_list,chunksize=chunksize):\n",
    "            result_list_tqdm.append(result)\n",
    "\n",
    "    return result_list_tqdm\n",
    "\n",
    "\n",
    "def run_IMAP_multiprocessing(func, argument_list, chunksize = None, show_prog = True):\n",
    "    \"\"\"Run function in parallel\n",
    "    Parameters\n",
    "    ----------\n",
    "    func:          function\n",
    "                    Python function to run in parallel.\n",
    "    argument_list: list [N]\n",
    "                    List of arguments to be passed to the function in each parallel run.\n",
    "            \n",
    "    show_prog:     boolean\n",
    "                    If true a progress bas will be displayed to show progress. Default: True.\n",
    "    Returns\n",
    "    -------\n",
    "    output:        list [N,]\n",
    "                    outputs of the function for the given arguments.\n",
    "    \"\"\"\n",
    "    #Reserve 2 threads for other Tasks\n",
    "    #pool = mp.Pool(processes=mp.cpu_count()-2)\n",
    "    \n",
    "    if show_prog:            \n",
    "        result_list_tqdm = []\n",
    "        for result in tqdm(pool.imap(func=func, iterable=argument_list,chunksize=chunksize), total=len(argument_list),position=0, leave=True):\n",
    "            result_list_tqdm.append(result)\n",
    "    else:\n",
    "        result_list_tqdm = []\n",
    "        for result in pool.imap(func=func, iterable=argument_list,chunksize=chunksize):\n",
    "            result_list_tqdm.append(result)\n",
    "\n",
    "    return result_list_tqdm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Format the Training Data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44, 2)\n"
     ]
    }
   ],
   "source": [
    "#Load XLimits\n",
    "\n",
    "X_LIMITS = np.load('/home/ada/Documents/HullParameterization/HullDiffusion/Restructured_Dataset/X_LIMITS.npy')\n",
    "\n",
    "print(X_LIMITS.shape)\n",
    "\n",
    "#Load in Volume Prediction for Now\n",
    "DesVecName = 'Input_Vectors.csv'\n",
    "YName = 'GeometricMeasures/Volume.csv'\n",
    "DS_path = '/home/ada/Documents//Hull_DataSet/'\n",
    "\n",
    "folder_roots = ['Constrained_Randomized_Set_1', \n",
    "                'Constrained_Randomized_Set_2',\n",
    "                'Constrained_Randomized_Set_3',\n",
    "                'Diffusion_Aug_Set_1',\n",
    "                'Diffusion_Aug_Set_2']\n",
    "DesVec = []\n",
    "\n",
    "for i in range(0,len(folder_roots)):\n",
    "    path = DS_path + folder_roots[i] + '/'    \n",
    "    #Location of Design Vectors\n",
    "    with open(path + DesVecName) as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for count, row in enumerate(reader):\n",
    "            if count != 0:\n",
    "                DesVec.append(row)\n",
    "'''\n",
    "    #Location of Vol Vectors\n",
    "    with open(path + YName) as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for count, row in enumerate(reader):\n",
    "            if count != 0:\n",
    "                YVec.append(row)\n",
    "            else:\n",
    "                labels = np.array(row)\n",
    "'''\n",
    "DesVec = np.array(DesVec)\n",
    "DesVec = DesVec.astype(np.float32())\n",
    "\n",
    "np.save('DesVec_82k.npy',DesVec)\n",
    "\n",
    "#YVec = np.array(YVec)\n",
    "#YVec = YVec.astype(np.float32())\n",
    "\n",
    "#Normalize Volume to LogScale\n",
    "#Y_log = np.log10(YVec)\n",
    "\n",
    "def Performance_Metric(X):\n",
    "    hull = HP(X)\n",
    "    Z = hull.Calc_VolumeProperties(101,1000)\n",
    "    \n",
    "    return np.divide(hull.Volumes,X[0]**3.0)\n",
    "\n",
    "def Calc_GeometricProperties(x):\n",
    "    '''\n",
    "    This function takes in a Ship Design Vector and calculates the volumetric properties of the hull \n",
    "    \n",
    "    It returns the values for:\n",
    "    \n",
    "    Z / L             -> nondimensialized vector for the height at which each value was measured\n",
    "    Volume / L^3\n",
    "    Area of Waterplane / L^2\n",
    "    Longitudinal Centers of Buoyancy/L\n",
    "    Vertical Center of Buoyancy / L\n",
    "    Longitudinal Center of Flotation / L\n",
    "    Ixx / L^4\n",
    "    Iyy / L^4\n",
    "    \n",
    "    where L = LOA of the design vector ( x[0])\n",
    "    \n",
    "    This function is written to be paralellized   \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    hull = HP(x)\n",
    "       \n",
    "    Z = hull.Calc_VolumeProperties(NUM_WL = 101, PointsPerWL = 1000)\n",
    "    \n",
    "    L = x[0]\n",
    "    \n",
    "    z = np.divide(Z,L)\n",
    "    Vol = np.divide(hull.Volumes,L**3.0)\n",
    "    WP = np.divide(hull.Areas_WP,L**2.0)\n",
    "    LCF = np.divide(hull.LCFs,L)\n",
    "    Ixx = np.divide(hull.I_WP[:,0],L**4.0)\n",
    "    Iyy = np.divide(hull.I_WP[:,1],L**4.0)\n",
    "    LCB = np.divide(hull.VolumeCentroids[:,0],L)\n",
    "    VCB = np.divide(hull.VolumeCentroids[:,0],L)\n",
    "    WSA = np.divide(hull.Area_WS,L**2.0)\n",
    "    WL = np.divide(hull.WL_Lengths,L)\n",
    "    \n",
    "    \n",
    "    return np.concatenate((z,Vol,WP,LCB,VCB,LCF,Ixx,Iyy,WSA,WL),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets clean up X\n",
    "\n",
    "idx_BBFactors = [33,34,35,36,37]\n",
    "idx_BB = 31\n",
    "\n",
    "idx_SBFactors = [38,39,40,41,42,43,44]\n",
    "idx_SB = 32\n",
    "\n",
    "for i in range(0,len(DesVec)):\n",
    "    \n",
    "    DesVec[i,idx_BBFactors] = DesVec[i,idx_BB] * DesVec[i,idx_BBFactors] \n",
    "    DesVec[i,idx_SBFactors] = DesVec[i,idx_SB] * DesVec[i,idx_SBFactors]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82168, 1010)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#Compute Y\n",
    "# Run Multiprocessing to Calculate the Geometric Measures\n",
    "CHUNKS = 256\n",
    "print('Calculating Hulls...')\n",
    "\n",
    "print('Threads: ' + str(mp.cpu_count()))\n",
    "pool = mp.Pool(processes=mp.cpu_count()-2)\n",
    "#Y = [Performance_Metric(DesVec[i]) for i in tqdm(range(0,len(DesVec)))]\n",
    "Y = run_IMAP_multiprocessing(Calc_GeometricProperties, DesVec,chunksize=CHUNKS,show_prog=True)\n",
    "Y = np.array(Y)\n",
    "np.save('GeometricMeasures.npy',Y)\n",
    "print('Hull Calculations Complete!')\n",
    "'''\n",
    "\n",
    "Y = np.load('GeometricMeasures.npy')\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantile Normalize the Design Vectors: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Normalizer:\n",
    "    def __init__(self, X_LL_Scaled, X_UL_Scaled,datalength):\n",
    "        \n",
    "        self.normalizer = PP.QuantileTransformer(\n",
    "            output_distribution='normal',\n",
    "            n_quantiles=max(min(datalength // 30, 1000), 10),\n",
    "            subsample=int(1e9)\n",
    "            )\n",
    "        \n",
    "        self.X_LL_Scaled = X_LL_Scaled\n",
    "        self.X_UL_Scaled = X_UL_Scaled\n",
    "        \n",
    "        self.X_LL_norm = np.zeros((1,len(X_LL_Scaled)))\n",
    "        self.X_UL_norm = np.zeros((1,len(X_LL_Scaled)))\n",
    "        \n",
    "        self.X_mean = np.zeros((1,len(X_LL_Scaled)))\n",
    "        self.X_std = np.zeros((1,len(X_LL_Scaled)))\n",
    "        \n",
    "    def fit_Data(self,X):\n",
    "        \n",
    "        \n",
    "        \n",
    "        x = 2.0*(X-self.X_LL_Scaled)/(self.X_UL_Scaled- self.X_LL_Scaled) - 1.0\n",
    "        \n",
    "        self.normalizer.fit(x)\n",
    "        x = self.normalizer.transform(x) # Scale Dataset between \n",
    "        #x = (X-self.X_LL_Scaled)/(self.X_UL_Scaled- self.X_LL_Scaled)\n",
    "        \n",
    "\n",
    "        return x\n",
    "    \n",
    "    def transform_Data(self,X):\n",
    "        x = 2.0*(X-self.X_LL_Scaled)/(self.X_UL_Scaled- self.X_LL_Scaled) - 1.0\n",
    "        \n",
    "        \n",
    "        x = self.normalizer.transform(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "    def scale_X(self,z):\n",
    "        #rescales data\n",
    "        z = self.normalizer.inverse_transform(z)\n",
    "        scaled = (z + 1.0) * 0.5 * (self.X_UL_Scaled - self.X_LL_Scaled) + self.X_LL_Scaled\n",
    "        #scaled = z* (self.X_UL_Scaled - self.X_LL_Scaled) + self.X_LL_Scaled\n",
    "\n",
    "        '''\n",
    "        x = self.normalizer.inverse_transform(x)\n",
    "        \n",
    "        #scaled = x* (self.X_UL_norm - self.X_LL_norm) + self.X_LL_norm\n",
    "        '''\n",
    "        #z = (z + 1.0) * 0.5 * (8.0) + 4.0\n",
    "       \n",
    "        #scaled = z*self.X_std + self.X_mean\n",
    "        #scaled = self.normalizer.inverse_transform(scaled)\n",
    "        return scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Regression Model Class ##\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  \n",
    "\n",
    "\n",
    "\n",
    "class Regression_ResNet(torch.nn.Module):\n",
    "    def __init__(self, Reg_Dict):\n",
    "        nn.Module.__init__(self)\n",
    "        \n",
    "        self.xdim = Reg_Dict['xdim']\n",
    "        self.ydim = 1\n",
    "        self.tdim = Reg_Dict['tdim']\n",
    "        self.net = Reg_Dict['net']\n",
    "        \n",
    "        self.fc = nn.ModuleList()\n",
    "        \n",
    "        self.fc.append(self.LinLayer(self.tdim,self.net[0]))\n",
    "        \n",
    "        for i in range(1, len(self.net)):\n",
    "            self.fc.append(self.LinLayer(self.net[i-1],self.net[i]))\n",
    "            \n",
    "        self.fc.append(self.LinLayer(self.net[-1], self.tdim))\n",
    "        '''\n",
    "        #self.tc = nn.ModuleList()\n",
    "\n",
    "        #for i in range(0, len(self.net)):\n",
    "            self.tc.append(self.LinLayer(self.tdim,self.net[i]))\n",
    "        self.tc.append(self.LinLayer(self.tdim, self.tdim))\n",
    "        '''\n",
    "        self.finalLayer = nn.Sequential(nn.Linear(self.tdim, self.ydim))\n",
    "        \n",
    "    \n",
    "        self.X_embed = nn.Linear(self.xdim, self.tdim)\n",
    "        #self.T_embed = nn.Linear(self.ydim, self.tdim)\n",
    "       \n",
    "        \n",
    "    def LinLayer(self, dimi, dimo):\n",
    "        \n",
    "        return nn.Sequential(nn.Linear(dimi,dimo),\n",
    "                             nn.SiLU(),\n",
    "                             nn.LayerNorm(dimo),\n",
    "                             nn.Dropout(p=0.1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.X_embed(x)\n",
    "    \n",
    "        res_x = x\n",
    "\n",
    "        for i in range(0,len(self.fc)):\n",
    "            x = self.fc[i](x)\n",
    "        \n",
    "        x = torch.add(x,res_x)\n",
    "        x = self.finalLayer(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Training Environment ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor_Training_Env:\n",
    "    def __init__(self, Reg_Dict, DesVec, Y):\n",
    "\n",
    "        self.Reg_Dict = Reg_Dict\n",
    "        self.DesVec = DesVec\n",
    "\n",
    "        self.QT = Data_Normalizer(X_LIMITS[:,0],X_LIMITS[:,1],len(DesVec))\n",
    "        \n",
    "        self.X = np.copy(DesVec[:,1:])\n",
    "\n",
    "        # Quantile Transform X:\n",
    "        self.X = self.QT.fit_Data(self.X)\n",
    "\n",
    "        self.Y = np.copy(Y)\n",
    "      \n",
    "        self.model = Regression_ResNet(self.Reg_Dict)\n",
    "        self.device =torch.device(self.Reg_Dict['device_name'])\n",
    "\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        self.data_length = len(self.X)\n",
    "        self.batch_size = self.Reg_Dict['batch_size']\n",
    "        self.num_epochs = self.Reg_Dict['Training_Epochs']\n",
    "        \n",
    "        lr = self.Reg_Dict['lr']\n",
    "        self.init_lr = lr\n",
    "        weight_decay = self.Reg_Dict['weight_decay']\n",
    "        \n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "    '''\n",
    "    ==============================================================================\n",
    "    Base Regression Training Functions\n",
    "    ==============================================================================\n",
    "    '''\n",
    "    \n",
    "    def run_regressor_step(self,x,y):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        ones = torch.ones_like(y)\n",
    "\n",
    "        predicted_y = self.model(x)\n",
    "        \n",
    "        loss =  F.mse_loss(predicted_y, y)\n",
    "        #print(loss)\n",
    "        \n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss  \n",
    "    \n",
    "    def run_train_regressors_loop(self,batches_per_epoch=64, subsample_per_batch = 64, num_WL_Steps = 101):\n",
    "            \n",
    "            num_batches = self.data_length // self.batch_size\n",
    "        \n",
    "            batches_per_epoch = min(num_batches,batches_per_epoch)\n",
    "\n",
    "            T_vec = np.linspace(0,1,num_WL_Steps)\n",
    "            \n",
    "            print('Regressor Model Training...')\n",
    "\n",
    "            for i in tqdm(range(0,self.num_epochs)):\n",
    "\n",
    "                for j in range(0,batches_per_epoch):\n",
    "                    \n",
    "                    A = np.random.randint(0,self.data_length,self.batch_size)\n",
    "                    x_batch = torch.tensor(self.X[A]).float().to(self.device) \n",
    "\n",
    "                \n",
    "                    for k in range(0,subsample_per_batch):\n",
    "                        #Random Waterline\n",
    "                        t = np.random.randint(0,num_WL_Steps,(self.batch_size,))\n",
    "                        t_tens = torch.tensor(T_vec[t,np.newaxis]).float().to(self.device)\n",
    "\n",
    "                        \n",
    "                        #Interpolate Volume\n",
    "                        Y_calc = np.array([HP.interp(y[i],T_vec,t[i]) for i in range(0,len(t))])\n",
    "\n",
    "\n",
    "                        y = self.Y[A,t]\n",
    "                        y_batch = torch.tensor(y[:,np.newaxis]).float().to(self.device)\n",
    "\n",
    "                        \n",
    "\n",
    "                        x = torch.cat((x_batch,t_tens),dim=1)\n",
    "\n",
    "                        loss = self.run_regressor_step(x,y_batch)\n",
    "                if i % 1000 == 0:\n",
    "                    print('Epoch: ' + str(i) + ' Loss: ' + str(loss))   \n",
    "                    \n",
    "        \n",
    "            print('Regression Model Training Complete!')\n",
    "\n",
    "            self.model.eval()\n",
    "            eval_size = 10000\n",
    "\n",
    "            A = np.random.randint(0,self.data_length,eval_size)\n",
    "\n",
    "            t = np.random.random((eval_size,1))\n",
    "            t_tens = torch.tensor(t).float().to(self.device)\n",
    "\n",
    "            x_eval = torch.tensor(self.X[A]).float().to(self.device)\n",
    "            \n",
    "            x_eval = torch.cat((x_eval, t_tens),dim=1) \n",
    "\n",
    "            Y_pred = self.model(x_eval)\n",
    "            Y_pred = Y_pred.to(torch.device('cpu')).detach().numpy() \n",
    "\n",
    "            y = self.Y[A]\n",
    "\n",
    "            Y_calc = np.array([HP.interp(y[i],T_vec,t[i]) for i in range(0,len(t))])\n",
    "\n",
    "            Rsq = r2_score(Y_calc, Y_pred)\n",
    "            print(\"R2 score of Y:\" + str(Rsq))\n",
    "\n",
    "\n",
    "    # SAVE FUNCTIONS\n",
    "        \n",
    "    def load_trained_model(self):\n",
    "        label = self.Reg_Dict['Model_Path']\n",
    "        self.model.load_state_dict(torch.load(label))\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "    \n",
    "    def Save_model(self,PATH):\n",
    "        '''\n",
    "        PATH is the path to the folder to store this in, including '/' at the end\n",
    "       \n",
    "        '''\n",
    "\n",
    "        torch.save(self.model.state_dict(), PATH + self.Reg_Dict['Model_Label']+'.pth')\n",
    "        \n",
    "        JSON = json.dumps(self.Reg_Dict)\n",
    "        f = open(PATH + self.Reg_Dict['Model_Label'] + '_Dict.json', 'w')\n",
    "        f.write(JSON)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Model Training ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([], dtype=int64), array([], dtype=int64))\n",
      "(82168, 101)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regressor Model Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/30000 [00:00<25:00, 20.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: tensor(0.1320, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1003/30000 [00:47<22:58, 21.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 Loss: tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2005/30000 [01:35<22:05, 21.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2000 Loss: tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3004/30000 [02:22<21:16, 21.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3000 Loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4003/30000 [03:09<20:31, 21.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4000 Loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5005/30000 [03:57<19:41, 21.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5000 Loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6004/30000 [04:44<18:57, 21.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6000 Loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7003/30000 [05:31<18:17, 20.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7000 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8005/30000 [06:19<17:15, 21.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8000 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9004/30000 [07:06<16:47, 20.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9000 Loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10005/30000 [07:53<15:42, 21.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10000 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 11004/30000 [08:41<15:03, 21.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11000 Loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12003/30000 [09:28<14:23, 20.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12000 Loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 13005/30000 [10:16<13:24, 21.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13000 Loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 14004/30000 [11:03<12:57, 20.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14000 Loss: tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15003/30000 [11:51<11:50, 21.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 16005/30000 [12:38<11:04, 21.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16000 Loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 17004/30000 [13:25<10:08, 21.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17000 Loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18003/30000 [14:13<09:28, 21.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18000 Loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 19005/30000 [15:00<08:41, 21.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19000 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20004/30000 [15:47<07:57, 20.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20000 Loss: tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21003/30000 [16:35<07:07, 21.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21000 Loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 22005/30000 [17:22<06:16, 21.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22000 Loss: tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 23004/30000 [18:10<05:31, 21.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23000 Loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24003/30000 [18:57<04:44, 21.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24000 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 25005/30000 [19:44<03:56, 21.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25000 Loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26004/30000 [20:32<03:06, 21.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26000 Loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27003/30000 [21:19<02:21, 21.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27000 Loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 28005/30000 [22:06<01:33, 21.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28000 Loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 29004/30000 [22:54<00:47, 21.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29000 Loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [23:41<00:00, 21.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Model Training Complete!\n",
      "R2 score of Y:0.9939168330224027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Regression model Dict\n",
    "nodes = 512\n",
    "\n",
    "Reg_Dict = {\n",
    "        'xdim' : len(DesVec[0])-1 + 1,              # Dimension of parametric design vector\n",
    "        'ydim': 1,                              # trains regression model for each objective\n",
    "        'tdim': nodes,                            # dimension of latent variable\n",
    "        'net': [nodes,nodes,nodes],                       # network architecture        \n",
    "        'Training_Epochs': 30000,               # number of training epochs\n",
    "        'batch_size': 1024,                       # batch size\n",
    "        'Model_Label': 'Regressor_WSA',         # labels for regressors\n",
    "                    \n",
    "        'lr' : 0.001,                          # learning rate\n",
    "        'weight_decay': 0.0,                   # weight decay\n",
    "        'device_name': 'cuda:0'}    \n",
    "\n",
    "\n",
    "num_WL_Steps = 101\n",
    "\n",
    "Y_set = np.log10(Y[:,8*num_WL_Steps:9*num_WL_Steps])\n",
    "#Y_set = Y[:,8*num_WL_Steps:9*num_WL_Steps]\n",
    "idx = np.where(np.isnan(Y_set))\n",
    "print(idx)\n",
    "\n",
    "Y_set[idx] = -6.0 #fix nan to dummy value\n",
    "\n",
    "print(Y_set.shape)\n",
    "\n",
    "REG = Regressor_Training_Env(Reg_Dict, DesVec,Y_set)\n",
    "\n",
    "REG.run_train_regressors_loop(batches_per_epoch=8, subsample_per_batch = 8, num_WL_Steps = num_WL_Steps)\n",
    "\n",
    "REG.Save_model('./TrainedModels/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log scale MAEP: 3.7725849970698446%\n",
      "Scaled MAEP: 6.909024703541988%\n",
      "R2 score of Scaled WSA Prediction: 0.986877063170483\n"
     ]
    }
   ],
   "source": [
    "REG.model.eval()\n",
    "\n",
    "sample_size = 100000\n",
    "\n",
    "T_vec = np.linspace(0,1,num_WL_Steps)\n",
    "A = np.random.randint(0,len(DesVec),sample_size)\n",
    "\n",
    "t = np.random.random((sample_size,1))\n",
    "t_tens = torch.tensor(t).float().to(REG.device)\n",
    "\n",
    "x_eval = torch.tensor(REG.X[A]).float().to(REG.device)\n",
    "\n",
    "x_eval = torch.cat((x_eval, t_tens),dim=1) \n",
    "\n",
    "Y_pred = REG.model(x_eval)\n",
    "Y_pred = Y_pred.to(torch.device('cpu')).detach().numpy() \n",
    "\n",
    "y = REG.Y[A]\n",
    "\n",
    "Y_calc = np.array([HP.interp(y[i],T_vec,t[i]) for i in range(0,len(t))])\n",
    "\n",
    "#MAEP = np.mean(np.abs(np.power(10,Y_calc)-np.power(10,Y_pred)/np.power(10,Y_calc)))\n",
    "MAEP = np.mean(np.abs(Y_calc-Y_pred)/np.abs(Y_calc))\n",
    "\n",
    "Y_scaled_calc = 10**Y_calc\n",
    "Y_scaled_pred = 10**Y_pred  \n",
    "\n",
    "\n",
    "print('Log scale MAEP: ' + str(MAEP*100.0) + '%')\n",
    "\n",
    "MAEP_scaled = np.mean(np.abs(Y_scaled_calc-Y_scaled_pred)/np.abs(Y_scaled_calc))\n",
    "print('Scaled MAEP: ' + str(MAEP_scaled*100.0) + '%')\n",
    "Rsq = r2_score(Y_scaled_calc, Y_scaled_pred)\n",
    "\n",
    "print(\"R2 score of Scaled WSA Prediction: \" + str(Rsq))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
